---
title: "LMTP_TargetedLearning"
output: html_document
---

```{r}
devtools::load_all(".")
# library(modulatR)

# make df 

# ---- Setup ----
set.seed(2025)
library(simcausal)
library(TargetedLearning)
library(dplyr)

# n: sample size
n <- 500

# ---- DAG specification in simcausal ----
D <- DAG.empty()

# Baseline covariates
D <- D +
  node("W1", distr = "rnorm", mean = 0, sd = 1) +
  node("W2", distr = "rbern", prob = 0.5)

# Two time-varying covariates at each t = 1,2,3
# Using a single node name with t=1:3 and then we'll rename to L11, L21, L31 etc.
D <- D +
  node("L1", t = 1:3, distr = "rnorm",
       mean =  0.8*W1 + 0.5*W2 + 0.2*t, sd = 1) +
  node("L2", t = 1:3, distr = "rnorm",
       mean = -0.3*W1 + 0.7*W2 - 0.1*t, sd = 1)

# Exposures at t = 1,2,3 (continuous here; switch to rbern with plogis if you want binary)
D <- D +
  node("A", t = 1:3, distr = "rnorm",
       mean =  0.6*L1[t] - 0.4*L2[t] + 0.4*W1 + 0.2*W2,
       sd = 1)

# Outcome depends on last-time covariates and all A's (feel free to change)
D <- D +
  node("Y", distr = "rnorm",
       mean =  0.7*L1[3] - 0.5*L2[3] + 0.4*A[1] + 0.3*A[2] + 0.2*A[3] + 0.3*W1 + 0.1*W2,
       sd = 1, t = 4)

# Finalize and simulate
Dset <- set.DAG(D)
raw <- sim(Dset, n = n)

# ---- Post-process to your exact column names ----
df <- raw %>%
  transmute(
    id = seq_len(n),
    W1 = W1, W2 = W2,
    L11 = L1_1, L12 = L2_1,
    L21 = L1_2, L22 = L2_2,
    L31 = L1_3, L32 = L2_3,
    A1  = A_1,  A2  = A_2,  A3  = A_3,
    Y = Y_4
  )

# Quick sanity check: structure & a peek
str(df)
dplyr::as_tibble(df, .name_repair = "minimal") |> dplyr::glimpse()



# put df into an LMTP data struct
ds <- LMTP_Data_Struct2$new(
  data = df, id_col = "id", n_timesteps = 3,
  A_cols = c("A1","A2","A3"),
  L_cols = list(c("L11","L12"), c("L21","L22"), c("L31","L32")),
  W_cols = c("W1","W2"),
  Y_col = "Y"
)


# define a sequence of policies 
shift_mtp <- mtp_additive_shift(delta = -.05)

policy_seq <- repeat_policy_over_time(shift_mtp, 3)

# set up the nuisance factories
# nuis <- LMTPNuisanceFactory$new(
#   # learners_Q = list(nadir::lnr_lm, nadir::lnr_rf, nadir::lnr_xgboost),
#   learners_g = list(lm = nadir::lnr_lm_density, homo1 = nadir::lnr_homoskedastic_density),
#   learners_g_extra_args = list(
#     homo1 = list(mean_lnr = nadir::lnr_earth)),
#   policy_seq = policy_seq,
#   A_type = "continuous",
#   repeat_fmls_lnrs_args = TRUE
# )

# simpler  models
nuis <- LMTPNuisanceFactory$new(
  # learners_Q = list(nadir::lnr_lm, nadir::lnr_rf, nadir::lnr_xgboost),
  learners_g = nadir::lnr_lm_density,
  policy_seq = policy_seq,
  A_type = "continuous",
  repeat_fmls_lnrs_args = TRUE
)


fit <- fit_tmle_for_LMTP(
  ds,
  policy_seq = policy_seq,
  learners_Q = nadir::lnr_lm,
  learners_g_factory = nuis,
  outcome_link = 'identity',
  repeat_lnrs = TRUE)


fit$psi; fit$ci95



identity_mtp <- mtp_additive_shift(delta = 0)

policy_seq_identity <- repeat_policy_over_time(identity_mtp, 3)

# simpler  models
nuis2 <- LMTPNuisanceFactory$new(
  # learners_Q = list(nadir::lnr_lm, nadir::lnr_rf, nadir::lnr_xgboost),
  learners_g = nadir::lnr_lm_density,
  policy_seq = policy_seq_identity,
  A_type = "continuous",
  repeat_fmls_lnrs_args = TRUE
)


fit <- fit_tmle_for_LMTP(
  ds,
  policy_seq = policy_seq_identity,
  learners_Q = nadir::lnr_lm,
  learners_g_factory = nuis2,
  outcome_link = 'identity',
  repeat_lnrs = TRUE)


fit$psi; fit$ci95
```


Let's try to just do outcome regression by itself on our own. 

```{r}
Q_trainer <- LMTPQTrainer$new(
  learners_Q = nadir::lnr_lm
)

Q_trainer(ds, t = 3, ds$Y()
```
