---
title: "Simulation of Simple Dosage LMTP"
fontsize: 9pt
format: pdf
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
# library(modulatR)
devtools::load_all(".")
library(tidyverse)
library(magrittr)
library(patchwork)
```

Outline: 
  
  * The purpose of this document is to provide some simple demonstration examples for 
  this package. 
  <!-- * We will consider two very simple scenarios in which taking more of the hypothetical
  drug has a positive effect. --> 
  * We will consider a very simple scenario in which taking more of the hypothetical
  drug has a positive effect.
  * In the first example, we will consider an additive shift effect. 
  <!-- * In the second example, we will consider a multiplicative shift. -->

## Simple Net-Benefit Effect, No Time-Varying Confounder/Exposure Feedback

Let's imagine as simple a scenario we can to test the implementation of this package.

  * There is a drug we are studying for its effect on some disease state 
  * The more of the drug you take, the better your disease state becomes! (Simple!) 
  * Drug dosage will be measured in milligrams with 1000mg being the typical dosage 
  each patient receives. 
  * The hypothetical policy is "What if we could get everyone to take 200mg more than 
  they would otherwise?"
  * Higher disease state = worse; lower = better. 
  * There are time-varying covariates $L_{i,t}$ representing disease state, but 
  that's it. 
  * Everyone in the study starts out with a fairly high disease state.
  * We will introduce the policy up-front so that we can model both factual 
  and counterfactual outcomes in the data generating process.
  
### Modified Treatment Policy Definition 

We'd like to know how much of an impact this might have on their outcomes. 

```{r}
extra_dosage_shift <- 200

policy <- MTP$new(
 smooth_invertible_regions = list(function(A, L) { TRUE }),
 policy = list(\(A, L) { A + extra_dosage_shift }),
 inverse_policy = list(\(A, L) { A - extra_dosage_shift }),
 derivative_of_policy = list(\(A, L) {1})
)
```

### Data Generating Process

```{r}
#| fig-width: 8
#| fig-height: 5
#| out-width: "100%"
#| fig-dpi: 300
# hyperparameters for study 
dosage_sd <- 100
n_timesteps <- 2L
n_individuals <- 1000L
avg_starting_disease_state <- 100
starting_disease_state_noise <- 0
avg_starting_dosage <- 1000
update_L_noise <- 0
update_A_noise <- 10

# the amount of reduction in disease state per day given their last dose received
dosage_effect <- function(A) {
  (A - 900) / 1000 * 50  
}

# generate some data
dgp_simple_scenario <- function(n, t, policy) {
  df <- list()
  stopifnot(is.integer(t))
  
  for (i in 1:(t+1)) {
    if (i == 1) {
      
      newA <- rnorm(n = n, mean = avg_starting_dosage, sd = dosage_sd)
      newdata <- 
        tibble::tibble(
          id = 1:n,
          t = 1,
          L = rnorm(n = n, mean = avg_starting_disease_state, starting_disease_state_noise),
          L_intervened = L,
          A = newA,
          A_intervened = policy$apply_policy(newA, L)
        )
    } else if (i > 1 && i < t+1) {
      prior_timestep <- 
        df |> dplyr::filter(t == i - 1)
      
      U_L <- rnorm(n = n_individuals, mean = 0, sd = update_L_noise)
      U_A <- rnorm(n = n, mean = 0, sd = update_A_noise)
      
      newdata <- 
        prior_timestep |> 
        dplyr::mutate(
          t = i,
          L = L + U_L - dosage_effect(prior_timestep$A),
          L_intervened = prior_timestep$L_intervened + U_L - dosage_effect(prior_timestep$A_intervened),
          A = prior_timestep$A + U_A) |> 
        mutate(
          A_intervened = policy$apply_policy(A_intervened, L_intervened)
        )
    } else if (i == t+1) {
      prior_timestep <- 
        df |> dplyr::filter(t == i - 1)
      
      U_L <- rnorm(n = n_individuals, mean = 0, sd = update_L_noise)
      
      newdata <- 
        prior_timestep |> 
        dplyr::mutate(
          t = i,
          L = L + U_L - dosage_effect(prior_timestep$A),
          L_intervened = prior_timestep$L_intervened + U_L - dosage_effect(prior_timestep$A_intervened),
          A = NA,
          A_intervened = NA
        )
    }
    
    df <- dplyr::bind_rows(df, newdata)
  }
  
  # the pseudo-outcome represents "what if the intervention had been carried out
  # during timesteps {i, ..., t}
  pseudo_outcomes <- list()
  for (i in 1:t) {
    
    # initial conditions; 
    # these are the conditions before the first update
    initial_timestep <- df |> filter(t == i) 
    unmodified_L <- initial_timestep |> pull(L)
    unmodified_A <- initial_timestep |> pull(A)
    
    for (j in i:(t+1)) {
      
      # we're going to need to modify the treatment received
      A_received_at_time_j <- df |> filter(t == j) |> pull(A)
      
      # if it's the first timestep, construct the pseudo_Lj as the [pseudocode]:
      # unmodified_L + dosage_effect(modified_treatment_Aj)
      if (j == i) {
        pseudo_Lj <- unmodified_L#  - dosage_effect(pseudo_Aj)
        pseudo_Aj <- policy$apply_policy(unmodified_A, unmodified_L)
      } else {
        # if it's a later step, take the prior_pseudo_Lj and update it with 
        # the dosage effect from the last timestep (under intervention)
        pseudo_Lj <- prior_pseudo_Lj - dosage_effect(prior_pseudo_Aj)
        
        # assign updated treatment according to policy
        if (j < t+1) {
          pseudo_Aj <- policy$apply_policy(A_received_at_time_j, pseudo_Lj)
        } else {
          pseudo_Aj <- NA # no treatment is assigned at time t+1
        }
      }
      
      # store pseudo_Lj and pseudo_Aj in prior_* in preparation for next timestep
      prior_pseudo_Lj <- pseudo_Lj
      prior_pseudo_Aj <- pseudo_Aj
      
      if (j == t+1) {
        pseudo_outcome_vec <- pseudo_Lj
      }
      rm(pseudo_Lj) # just to make sure no mistakes are introduced
      rm(pseudo_Aj)
    }
    pseudo_outcomes[[length(pseudo_outcomes) + 1]] <-
      tibble::tibble(id = initial_timestep$id,
        t = i,
        pseudo_outcome = pseudo_outcome_vec)
  }
  pseudo_outcomes <- dplyr::bind_rows(pseudo_outcomes)
  
  df <- left_join(df, pseudo_outcomes, by = c('id' = 'id', 't' = 't'))
  
  return(df)
}

df <- dgp_simple_scenario(n = n_individuals, t = n_timesteps, policy = policy)

color_range <- range(c(df$A_intervened, df$A), na.rm = TRUE)
y_range <- range(c(df$L, df$L_intervened), na.rm = TRUE)

matching_theme <- 
  theme_bw() + 
  theme(legend.position = 'bottom',
        panel.grid = element_blank(),
        legend.title.position = 'top',
        legend.key.width = unit(1,'cm')) 

matching_labs <- 
  labs(
    x = "Time",
    y = "Disease State",
    color = "Dosage Taken",
    shape = "Dosage Taken",
    subtitle = paste0("n = ", n_individuals, ", ", n_timesteps, " time-steps"))

matching_color_scale <- 
  scale_color_viridis_c(option = 'plasma', limits = color_range) 

plt1 <- ggplot(df, aes(x = t, y = L, group = id, color = A)) + 
  geom_point() + 
  geom_line(alpha = 0.5, linewidth = 1) + 
  ggtitle("Baseline Scenario\nPatient History in Simulation Study") + 
  ylim(y_range[1], y_range[2]) + 
  matching_color_scale + 
  matching_labs + 
  matching_theme

plt2 <- ggplot(df, aes(x = t, y = L_intervened, group = id, color = A_intervened)) + 
  geom_point() + 
  geom_line(alpha = 0.5, linewidth = 1) + 
  ggtitle("Intervention Scenario\nPatient History in Simulation Study") + 
  ylim(y_range[1], y_range[2]) + 
  matching_color_scale + 
  matching_labs + 
  matching_theme

{ plt1 + plt2 } +
  plot_layout(guides = 'collect') & 
  theme(legend.position = 'bottom')

ggplot(df, aes(x = t, y = pseudo_outcome, group = id)) + 
  geom_point() + 
  geom_line() + 
  geom_point(
    data = df |> filter(t == n_timesteps + 1),
    mapping = aes(x = 0, y = L_intervened),
    shape = 2) + 
  theme_bw() 
```

#### True SATE 

At this point, we can evaluate what the true sample average treatment effect was
since we simulated both the realized and counterfactual outcomes. 

```{r}
df |> 
  filter(t == n_timesteps + 1) |> 
  mutate(Y_diff = L - L_intervened) |> 
  pull(Y_diff) |>
  summary()

# this should match given the definition of the dosage_effect
((avg_starting_dosage + extra_dosage_shift - 900) / 1000 * 50 - 
    (avg_starting_dosage-900)/1000 * 50) * n_timesteps
```


```{r}
# reshaping to a wide format for lmtpi 
df_wide <- df |> 
  tidyr::pivot_wider(
    id_cols = 'id',
    names_from = 't',
    values_from = c('L', 'A'))

# name outcome properly and 
# drop extraneous treatment variable for A_5 (NAs from pivoting)
df_wide %<>% rename(Y = paste0("L_", n_timesteps+1))
df_wide %<>% select(-paste0("A_", n_timesteps+1))

lmtp_data <- LMTP_Data_Struct$new(
  data = df_wide,
  id_col = 'id',
  n_timesteps = n_timesteps,
  exposure_cols = paste0('A_', 1:n_timesteps),
  time_varying_covariate_cols = lapply(1:n_timesteps, \(t) { paste0('L_', t) }),
  outcome_col = 'Y'
)
```

## outcome regression 

To quote the text of the DÃ­az et al 2023 paper: 

> In particular, $m_t : t = 1, ..., \tau$ may be estimated as follows. Start by
running a preferred regression algorithm of $m_{\tau+1, i} = Y_i$ on
$(A_{\tau,i}, H_{\tau, i})$. Then evaluate the estimator $\hat m_{\tau}$ at
$(A_{\tau, i}^{\mathbb{d}}, H_{\tau, i})$, that is, compute the prediction $\hat
m_{\tau}(A_{\tau, i}^{\mathbb d}, H_{\tau, i})$. Use this prediction as the
pseudo-outcome in a regression on $(A_{\tau-1,i}, H_{\tau, i})$ to obtain an
estimate of $\hat m_{\tau-1}$. Compute the pseudo-outcome $\hat
m_{\tau-1}(A_{\tau-1,i}, H_{\tau, i})$ and iterate the process until obtaining
an estimate for $\hat m_1$.

```{r}
# name of the y-variable; in this case just 'Y' we get the name of the
# Y-variable so that we can build regressions that consistently use the name for
# Y as the outcome, even for the pseudo-outcome regressions so that if the
# user/programmer looks into the models fit, the meaning of the outcome variable
# is clear.
Y_col <- lmtp_data$outcome_col

for (timestep_t in (n_timesteps):1) {
  print(paste0('time: ', timestep_t))
  
  # get history up to time t 
  # Ht includes (L1, ..., Lt, A1, ..., A_{t-1})
  # We can think of Ht as the history up to (before, not including) At 
  Ht = lmtp_data$H(timestep_t)
  Lt = lmtp_data$L(timestep_t)
  
  if (timestep_t == n_timesteps) { # for the first g-computation step
    
    # get the full history of exposures and covariates (At, Ht)
    training_dataset <- 
      dplyr::bind_cols(
           Y = lmtp_data$Y,
           A = lmtp_data$A(timestep_t),
           Lt)
           # Ht)
    
    
  } else if (timestep_t < n_timesteps & timestep_t >= 1) { # for subsequent g-computation steps
    
    # form a training dataset using the pseudo-outcome, At, Ht
    training_dataset <-
      dplyr::bind_cols(Y = pseudo_outcome,
                       A = lmtp_data$A(timestep_t),
                       Lt)
                       # Ht)
  }
  
  # make sure everything is named properly 
  A_col = lmtp_data$exposure_cols[timestep_t]
  colnames(training_dataset)[1] <- Y_col
  colnames(training_dataset)[2] <- A_col
  
  # regress Y on everything else in the training data
  model_t <- 
    lm(formula = paste0(Y_col, " ~ ."), data = training_dataset)
  
  # use the MTP to apply the policy to A
  mtp_shifted_A_t <- policy$apply_policy(A = lmtp_data$A(timestep_t), L = lmtp_data$L(timestep_t)) # doesn't depend on L
  
  # combine into a dataframe of predictor variables to predict on
  prediction_dataset_under_policy_shift <- dplyr::bind_cols(A_col = mtp_shifted_A_t, Lt) # Ht)
  
  # make sure the At variable is named properly 
  colnames(prediction_dataset_under_policy_shift)[1] <- A_col
  
  # predict under the intervention at time t
  pseudo_outcome <- predict(model_t, newdata = prediction_dataset_under_policy_shift)
  # pseudo_outcome <- predict(model_t, newdata = training_dataset)
}

# estimated LMTP effect distribution
summary({ df_wide |> pull(Y)} - pseudo_outcome)

# estimated LMTP effect
mean({ df_wide |> pull(Y)}) - mean(pseudo_outcome)

# bias 
summary(df |> filter(t == n_timesteps + 1) |> pull(L_intervened) - pseudo_outcome)
```

<!-- 
## ipw estimate

## doubly robust estimation

```{r}
# lmtp_double_robust_estimator <- function(data, mtp, time, ...) {
#   # fitting treatment models 
# }
```

-->
