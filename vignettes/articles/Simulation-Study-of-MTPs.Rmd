---
title: "Simulation Study of MTPs"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
# library(modulatR)
devtools::load_all(".")
library(nadir)
library(Simulacron3)
library(dplyr)
library(ggplot2)
```

```{r simple dgp}
dgp <- function(n) {
  L1 <- rnorm(n = n)
  L2 <- rnorm(n = n, mean = 10, sd = 2)
  L3 <- runif(n = n, min = 0, max = 15)

  # simple:
  A <- 1.5 * L1 + rnorm(n = n, sd = 3)

  # complicated:
  # A <- exp((2.1 * L1 + .5 * L1 * L2 - 1.5 * L3) / 15) + rnorm(n = n, sd = 3)

  # simple:
  U_Y <- rnorm(n = n)
  Y <- 1.3 * A + .7 * L1 + 2.2 * L1*L2 + L3 + U_Y

  # complicated:
  # Y <- 1.25 * A + .8 * A^2 + L1 + 2.2 * L1*L2 + L3

  # simple:
  Y_intervened <-
    1.3 * (A-5) + .7 * L1 + 2.2 * L1*L2 + L3 + U_Y
  # complicated:
  # Y_intervened <-
  #   1.25 * (A-5) + .8 * (A-5)^2 + L1 + 2.2 * L1*L2 + L3

  data.frame(L1 = L1, L2 = L2, L3 = L3, A = A, Y = Y, Y_intervened)
}
```

```{r examples with simple dgp}
# examples: 
# head(dgp(100))
df <- dgp(1000)

# train the exposure super learner...

predict_df <- expand.grid(
  L1 = seq(-3, 3, length.out=100),
  A = seq(-10, 12, length.out=100))

predict_df$L2 <- mean(df$L2)
predict_df$L3 <- mean(df$L3)

df_without_Y <- df |> dplyr::select(-Y_intervened, -Y)

sl_learned_exposure_density <- 
  nadir::super_learner(
    data = df_without_Y,
    learners = list(
      lm = lnr_lm_density,
      rf = lnr_homoskedastic_density,
      earth = lnr_homoskedastic_density,
      glmnet = lnr_homoskedastic_density,
      glmnet2 = lnr_homoskedastic_density
    ),
    extra_learner_args = list(
      rf = list(mean_lnr = lnr_rf),
      earth = list(mean_lnr = lnr_earth),
      glmnet = list(mean_lnr = lnr_glmnet),
      glmnet2 = list(mean_lnr = lnr_glmnet, mean_lnr_args = list(lambda = 10))
      ),
    formula = A ~ L1 + L2 + L3 + L1*L2,
    outcome_type = 'density',
    verbose = TRUE)
  
predict_df$density <- sl_learned_exposure_density$sl_predictor(predict_df)

ggplot(df, aes(x = L1, y = A)) + 
  geom_tile(data = predict_df, mapping = aes(fill = density)) + 
  geom_point(alpha = .9) + 
  MetBrewer::scale_fill_met_c('Hiroshige', -1) + 
  ggtitle("The density model of A given L")
```


```{r double robust estimator}
doubly_robust_estimator <- function(df) {
  
  df_without_Y <- df |> dplyr::select(-Y_intervened, -Y)
  
  # treatment/exposure model
  sl_learned_exposure_density <- 
    nadir::super_learner(
      data = df_without_Y,
      learners = list(
        lm = lnr_lm_density,
        rf = lnr_homoskedastic_density,
        earth = lnr_homoskedastic_density,
        glmnet = lnr_homoskedastic_density,
        glmnet2 = lnr_homoskedastic_density
      ),
      extra_learner_args = list(
        rf = list(mean_lnr = lnr_rf),
        earth = list(mean_lnr = lnr_earth),
        glmnet = list(mean_lnr = lnr_glmnet),
        glmnet2 = list(mean_lnr = lnr_glmnet, mean_lnr_args = list(lambda = 10))
        ),
      formula = A ~ L1 + L2 + L3 + L1*L2,
      outcome_type = 'density',
      verbose = TRUE)
  
  # round(sl_learned_exposure_density$learner_weights, 3)
  
  # compare_learners(sl_learned_exposure_density, loss_metric = negative_log_loss)
  
  # sl_learned_exposure_density$sl_predictor(df)
  
  # integration check? 
  # the rationale is that a (conditional) density, if we fix the covariates,
  # should integrate to 1.
  # f <- function(As) {
  #   x <- df_without_Y[1,,drop=FALSE]
  #   # x <- x |> dplyr::select(-Y_intervened, -Y)
  #   sapply(As, function(a) {
  #     x[['A']] <- a
  #     sl_learned_exposure_density$sl_predictor(x)
  #   })
  # }
  
  # takes about ~15s but good to check:
  # integrate(f, lower = -5, upper = 5)
  
  df_without_Y_intervened <- df |> dplyr::select(-Y_intervened)
  
  # outcome model
  sl_learned_outcome <- 
    nadir::super_learner(
      data = df_without_Y_intervened,
      learners = list(
        lm = lnr_lm,
        lm2 = lnr_lm,
        lm3 = lnr_lm,
        rf = lnr_rf,
        earth = lnr_earth,
        glmnet = lnr_glmnet,
        glmnet2 = lnr_glmnet
      ),
      extra_learner_args = list(glmnet2 = list(lambda = 10)),
      formulas = list(
        .default = Y ~ A + L1 + L2 + L3,
        lm2 = Y ~ A + L1*L2 + L2 + L3,
        lm3 = Y ~ poly(A, 2) + L1 + L2 + L3 + L1 * L2
      ),
      verbose = TRUE)
  
  # round(sl_learned_outcome$learner_weights, 5)
  
  mtp <- MTP$new(
     smooth_invertible_regions = function(A, L) { return(rep(TRUE, length(A))) },
     policy = function(A, L) { A - 5 },
     inverse_policy = function(A, L) { A + 5 },
     derivative_of_policy = function(A, L) { 1 }
  )
  
  # just check to make sure the policy applies correctly
  # df$A - mtp$apply_policy(A = df[,'A'], L = df[,c('L1', 'L2', 'L3')])
  
  data_without_intervention <- df 
  
  data_with_intervention <-
    data_with_inverse_intervention <-
    data_without_intervention # copy data (and omit the outcome to prevent leaking)
  
  # and check to see the inverse intervention is working
  # data_without_intervention$A - data_with_inverse_intervention$A
  
  # setup intervention dataset
  data_with_intervention$A <- mtp$apply_policy(A = df[,'A'], L = df[,c('L1', 'L2', 'L3')])
  # setup inverse intervention dataset
  data_with_inverse_intervention$A <-
    mtp$apply_inverse_policy(A = df[, 'A'], L = df[, c('L1', 'L2', 'L3')])
  
  Yhat_without_intervention <- sl_learned_outcome$sl_predictor(data_without_intervention)
  
  Yhat_under_intervention <- sl_learned_outcome$sl_predictor(data_with_intervention)
  
  
  # outcome-regression style estimtae 
  mean(Yhat_without_intervention - Yhat_under_intervention)
  
  
  density_at_inverse_intervention <-
    sl_learned_exposure_density$sl_predictor(data_with_inverse_intervention)
  
  density_at_obs_exposure <- 
    sl_learned_exposure_density$sl_predictor({ data_without_intervention |> select(-Y_intervened, -Y) })
  
  weights <- density_at_inverse_intervention / density_at_obs_exposure
  
  
  weights <- weights / sum(weights) * nrow(df)
  
  # IPW -like estimator 
  reweighted_Y <- weights * df$Y * 1 # apply weights to Y
  IPW_effect_estimate <- mean(df$Y) - mean(reweighted_Y)
  
  # now we turn to the doubly robust estimation part given that we've above 
  # fit the outcome model and treatment model and gotten weights 
  # 
  # the doubly robust estimation procedure works along the lines of a targeting
  # step: 
  # 
  # we fit a regression that fits: 
  #   Y ~ gamma_hat * weights + Yhat  (with no intercept) 
  # where Yhat is the outcome regression model prediction for Y without any intervention.
  # 
  # Then we use gamma_hat to construct an improved estimate for Yhat_with_intervention
  # using new weights (weights that use d(A, L) instead of A)
  # 
  # Díaz and van der Laan refer to the original weights as H(A, L) and 
  # the second set of weights as H(d(A, L), L).
  
  # clever covariate model
  lm_model_to_determine_epsilon <- lm(data_without_intervention$Y ~ weights + offset(Yhat_without_intervention) + 0)
  
  # extract gamma_hat
  epsilon_hat <- coef(lm_model_to_determine_epsilon)[[1]]
  
  # Some notation from Haneuse and Rotnitzky: 
  #   They refer to Q(A, L) as the policy 
  #   And H as the inverse of Q. 
  #   They refer to lambda(A, L) as the function that produces the appropriate
  #   inverse probability weights.
  # 
  # Eqn 15 of Haneuse & Rotnitzky 2013 says to plug in Q into the lambda() function:
  # lambda(A) is the density ratio:  density(H(A), L) / density(A, L), 
  # so I think lambda(Q) = density(H(Q(A)) | L) / density(Q | L) 
  #     = density(A | L) / density(Q | L)
  #     = density(treatment received) / density(modified treatment)
  # 
  # Díaz and van der Laan write this slightly differently, calling the weights
  # function H(A, L) or even H(g)(A,L) where g is their notation for treatment
  # model
  weights_at_intervention <- density_at_obs_exposure / sl_learned_exposure_density$sl_predictor(data_with_intervention)
  
  effect_estimate <- mean(df$Y) - mean(Yhat_under_intervention + epsilon_hat * weights_at_intervention)

  
  # uncentered_influence_function <- weights * (obs_data$Y - Yhat_without_intervention) + (Yhat_under_intervention + epsilon_hat * weights_at_Q)
  # centered_influence_function <- uncentered_influence_function - effect_estimate
  # variance_estimate1 <- mean(centered_influence_function^2) / nrow(obs_data)
  
  # based on Ch 14.2 in Targeted Learning (2018) Causal Inference for Complex and Longitudinal Studies
  # the EIF is 
  # 
  # D(P)(o) = H(a,w){y - \bar{Q}(a, w)} + \bar{Q}(d(a,w), w) - \Psi(P),
  # where H(a,w) are the weights as specified above at the inverse intervention over 
  # the observed treatment for each of the piecewise segments.
  # 
  # \bar{Q} is the true outcome regression, so we plug-into it our learned regression model
  # and in their notation, d(a,w) is the modified/dynamic treatment assigned given treatment 
  # observed and covariates in their notation.
  # 
  eif <- 
    weights * (df$Y - Yhat_without_intervention) +
    Yhat_under_intervention - mean(Yhat_under_intervention) - (df$Y - mean(df$Y)) 
    
  # estimate the variance of the eif and divide by by n 
  variance_estimate <- var(eif) / nrow(df)
  
  # calculate asymptotic influence function based 95% confidence intervals
  ci95 <- effect_estimate + c(-1, 1) * qnorm(0.975) * sqrt(variance_estimate)
  
  return(c(
    estimate = effect_estimate,
    variance = variance_estimate,
    cihigh = ci95[2],
    cilow = ci95[1]
  ))
}
```

```{r some examples of calling the estimator}
doubly_robust_estimator(dgp(30))
doubly_robust_estimator(dgp(40))
doubly_robust_estimator(dgp(50))
```

```{r simple simulation}
library(future)

plan(sequential)
# or plan(multicore)

sim <- Simulacron3::Simulation$new()
sim$set_dgp(dgp)
sim$set_estimators(list(DR = doubly_robust_estimator))
sim$set_config(list(replications = 30, sample_size = 30, parallel = FALSE))

summary_fn <- function(i, est_results, data) {
  summary_df <- tibble::tibble(
    estimate = est_results$DR['estimate'],
    estimator_cihigh = est_results$DR['cihigh'],
    estimator_cilow = est_results$DR['cilow'],
    true_effect = mean(data$Y - data$Y_intervened)
  ) 
  
  summary_df |> dplyr::mutate(
    does_cover = true_effect %btn[]% c(estimator_cilow, estimator_cihigh),
    bias = true_effect - estimate
  )
}
sim$set_summary_stats(summary_fn)

# takes a couple minutes even for the simple model
set.seed(1234)

# keep getting an empty model matrix... why? 
# 
results <- list()
for (sample_size in c(30, 100, 300)) {
  sim$set_config(list(sample_size = sample_size, replications = 10))
  sim$run()
  results[[length(results)+1]] <- dplyr::bind_cols(sample_size = sample_size, sim$get_results())
}
results <- do.call(rbind, results)
```


```{r visualize simple simulation results}
ggplot(results, aes(x = factor(sample_size), y = estimate,
                    shape = 'Doubly Robust MTP Estimator')) + 
  geom_point() + 
  geom_point(
    position = position_nudge(x = .25),
    mapping = aes(y = true_effect, shape = 'True Effects (known from Simulation)'),
  ) + 
  geom_hline(linetype = 'dashed', color = 'firebrick', yintercept = 1.3 * 5) + 
  geom_boxplot(alpha = 0.5, width = .15, fill = NA) + 
  scale_shape_manual(values = c(
    'Doubly Robust MTP Estimator' = 1,
    'True Effects (known from Simulation)' = 2)) + 
  theme_bw() + 
  labs(
    title = 'Comparison of MTP Doubly Robust estimators to true effects'
  ) + 
  theme(legend.position = 'bottom')



ggplot(results, aes(x = factor(sample_size), y = estimate,
                    shape = 'Doubly Robust MTP Estimator')) + 
  geom_pointrange(mapping = aes(ymin = estimator_cilow, ymax = estimator_cihigh),
                  position = position_dodge2(width = .2), size = .25) + 
  geom_point(
    position = position_nudge(x = .25),
    mapping = aes(y = true_effect, shape = 'True Effects (known from Simulation)'),
  ) + 
  geom_hline(linetype = 'dashed', color = 'firebrick', yintercept = 1.3 * 5) + 
  scale_shape_manual(values = c(
    'Doubly Robust MTP Estimator' = 10,
    'True Effects (known from Simulation)' = 2)) + 
  theme_bw() + 
  labs(
    title = 'Comparison of MTP Doubly Robust estimators to true effects',
    subtitle = 'Each simulation estimate is shown with its 95% CI',
    x = "sample size"
  ) + 
  theme(legend.position = 'bottom')
```


# More Complicated Example 


```{r second dgp}
dgp2 <- function(n) {
  L1 <- rnorm(n = n)
  L2 <- rnorm(n = n, mean = 10, sd = 2)
  L3 <- runif(n = n, min = 0, max = 15)

  A <- exp((2.1 * L1 + .5 * L1 * L2 - 1.5 * L3) / 15) + ifelse(L1 > 0, 3, 0) + rnorm(n = n, sd = 3)

  U_Y <- rnorm(n = n)
  Y <- 1.25 * A + .8 * A^2 + L1 + 2.2 * L1*L2 + L3

  Y_intervened <-
    1.25 * (A-5) + .8 * (A-5)^2 + L1 + 2.2 * L1*L2 + L3 + U_Y

  data.frame(L1 = L1, L2 = L2, L3 = L3, A = A, Y = Y, Y_intervened)
}
```

```{r examples with complicated dgp2}
# examples: 
# head(dgp(100))
df <- dgp2(1000)

# train the exposure super learner...

predict_df <- expand.grid(
  L1 = seq(-3, 3, length.out=100),
  A = seq(-10, 12, length.out=100))

predict_df$L2 <- mean(df$L2)
predict_df$L3 <- mean(df$L3)

df_without_Y <- df |> dplyr::select(-Y_intervened, -Y)

sl_learned_exposure_density <- 
  nadir::super_learner(
    data = df_without_Y,
    learners = list(
      lm = lnr_lm_density,
      rf = lnr_homoskedastic_density,
      earth = lnr_homoskedastic_density,
      glmnet = lnr_homoskedastic_density,
      glmnet2 = lnr_homoskedastic_density
    ),
    extra_learner_args = list(
      rf = list(mean_lnr = lnr_rf),
      earth = list(mean_lnr = lnr_earth),
      glmnet = list(mean_lnr = lnr_glmnet),
      glmnet2 = list(mean_lnr = lnr_glmnet, mean_lnr_args = list(lambda = 10))
      ),
    formula = A ~ L1 + L2 + L3 + L1*L2,
    outcome_type = 'density',
    verbose = TRUE)
  
predict_df$density <- sl_learned_exposure_density$sl_predictor(predict_df)

ggplot(df, aes(x = L1, y = A)) + 
  geom_tile(data = predict_df, mapping = aes(fill = density)) + 
  geom_point(alpha = .9) + 
  MetBrewer::scale_fill_met_c('Hiroshige', -1) + 
  ggtitle("The density model of A given L")
```


```{r some examples of calling the estimator on the second dgp}
doubly_robust_estimator(dgp2(30))
doubly_robust_estimator(dgp2(40))
doubly_robust_estimator(dgp2(50))
```

```{r more complicated simulation}
plan(sequential)
# or plan(multicore)

sim <- Simulacron3::Simulation$new()
sim$set_dgp(dgp2)
sim$set_estimators(list(DR = doubly_robust_estimator))
sim$set_config(list(replications = 30, sample_size = 30, parallel = FALSE))

summary_fn <- function(i, est_results, data) {
  summary_df <- tibble::tibble(
    estimate = est_results$DR['estimate'],
    estimator_cihigh = est_results$DR['cihigh'],
    estimator_cilow = est_results$DR['cilow'],
    true_effect = mean(data$Y - data$Y_intervened)
  ) 
  
  summary_df |> dplyr::mutate(
    does_cover = true_effect %btn[]% c(estimator_cilow, estimator_cihigh),
    bias = true_effect - estimate
  )
}
sim$set_summary_stats(summary_fn)

# takes a couple minutes even for the simple model
set.seed(1234)

# keep getting an empty model matrix... why? 
# 
results <- list()
for (sample_size in c(30, 100, 300, 1000)) {
  sim$set_config(list(sample_size = sample_size, replications = 10))
  sim$run()
  results[[length(results)+1]] <- dplyr::bind_cols(sample_size = sample_size, sim$get_results())
}
results <- do.call(rbind, results)
```


```{r visualize complicated simulation results}
ggplot(results, aes(x = factor(sample_size), y = estimate,
                    shape = 'Doubly Robust MTP Estimator')) + 
  geom_point() + 
  geom_point(
    position = position_nudge(x = .25),
    mapping = aes(y = true_effect, shape = 'True Sample Average Treatment Effects (known from Simulation)'),
  ) + 
  geom_hline(linetype = 'dashed', color = 'firebrick', 
    yintercept = 1.3 * 5) + 
  geom_boxplot(alpha = 0.5, width = .15, fill = NA) + 
  scale_shape_manual(values = c(
    'Doubly Robust MTP Estimator' = 1,
    'True Sample Average Treatment Effects (known from Simulation)' = 2)) + 
  theme_bw() + 
  labs(
    title = 'Comparison of MTP Doubly Robust estimators to true effects',
    subtitle = 'Population average treatment effect shown by dashed line'
  ) + 
  theme(legend.position = 'bottom')



ggplot(results, aes(x = factor(sample_size), y = estimate,
                    shape = 'Doubly Robust MTP Estimator')) + 
  geom_pointrange(mapping = aes(ymin = estimator_cilow, ymax = estimator_cihigh),
                  position = position_dodge2(width = .2), size = .25) + 
  geom_point(
    position = position_nudge(x = .25),
    mapping = aes(y = true_effect, shape = 'True Sample Average Treatment Effects (known from Simulation)'),
  ) + 
  geom_hline(linetype = 'dashed', color = 'firebrick', yintercept = 1.3 * 5) + 
  scale_shape_manual(values = c(
    'Doubly Robust MTP Estimator' = 10,
    'True Sample Average Treatment Effects (known from Simulation)' = 2)) + 
  theme_bw() + 
  labs(
    title = 'Comparison of MTP Doubly Robust estimators to true effects',
    subtitle = 'Each simulation estimate is shown with its 95% CI',
    x = "sample size"
  ) + 
  theme(legend.position = 'bottom')
```

